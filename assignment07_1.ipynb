{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7.1 - Viola & Jones \n",
    "\n",
    "Welcome to the assignment for week 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please state both names of your group members here:\n",
    "Jane and John Doe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 7.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.1.1: Rectangular Features\n",
    "\n",
    "* Implement the construction of all 2-rect features (horizontal and vertical) within a given window (like the 24x24 px window from the paper)\n",
    "* How many exist? **(RESULT)** \n",
    "* Display three of these overlaid onto an image of a face. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, color, transform\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# In case you don't have scikit-image installed, uncomment the following line:\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.1.2: Integral Image\n",
    "\n",
    "* Implement a function that computes the integral image of a given input image. Display the integral image for an input of you choice **(RESULT)**. \n",
    "* Compare it to the output of skimage's integral_image() function. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.1.3: One Weak Classifier\n",
    "\n",
    "* Implement a weak classifier that uses a single rectangular feature to classify an image as face/non-face. The classifier should take as input the integral image, the feature type, position, size, and threshold. It should output 1 for face, and 0 for non-face predictions. **(RESULT)**\n",
    "* Test your weak classifier on an example image of your choice. **(RESULT)**\n",
    "\n",
    "Suggestion: Use one cifar image as non-face and one tinyface image as face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TinyFace using for example gdown\n",
    "# !pip install gdown\n",
    "# !gdown 1xTZc7lNmWN33ECO2AKH6FycGdiqIK7W0\n",
    "# !unzip tinyface.zip\n",
    "# !rm tinyface.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tinyface data\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def move_files_to_parent(parent_dir):\n",
    "    for root, dirs, files in os.walk(parent_dir, topdown=False):\n",
    "        # Move files in subdirectories to the parent directory\n",
    "        for file in files:\n",
    "            src = os.path.join(root, file)\n",
    "            dst = os.path.join(parent_dir, file)\n",
    "            shutil.move(src, dst)\n",
    "        \n",
    "        # Remove empty subdirectories\n",
    "        for directory in dirs:\n",
    "            subdir = os.path.join(root, directory)\n",
    "            if not os.listdir(subdir):  # Check if the directory is empty\n",
    "                os.rmdir(subdir)\n",
    "\n",
    "path = 'tinyface/Training_Set/'\n",
    "move_files_to_parent(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyFace(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory with all images.\n",
    "            transform (callable, optional): Optional transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_dir = img_dir\n",
    "        self.image_filenames = [\n",
    "            f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the image to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            image: Transformed image.\n",
    "            label: Label (optional, could be derived from filenames if needed).\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure 3-channel RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = 1\n",
    "        return (image, label)\n",
    "\n",
    "# Define the transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((24, 24)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "tinyface_dataset = TinyFace(img_dir='tinyface/Training_Set', transform=transform)\n",
    "\n",
    "# Access a single sample\n",
    "image, label = tinyface_dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "print(len(tinyface_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Data samples done\n",
    "# Now loading CIFAR-10 dataset as non-face data and prepping it similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset as non-face data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((24, 24)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Setting all targets to 0 (non-face)\n",
    "cifar10_train.targets = [0] * len(cifar10_train)\n",
    "cifar10_test.targets = [0] * len(cifar10_test)\n",
    "\n",
    "# Access samples\n",
    "image, label = cifar10_train[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.1.4: AdaBoosting (BONUS)\n",
    "\n",
    "* Implement the AdaBoost algorithm as outlined in the paper (https://www.face-rec.org/algorithms/Boosting-Ensemble/16981346.pdf).\n",
    "* Train a 20-feature classifier using these datasets:\n",
    "    - TinyFace for low resolution face images: https://qmul-tinyface.github.io/\n",
    "    - CIFAR10 for negative samples: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "* Test your classifier and report the results for different thresholds **(RESULTS)**\n",
    "\n",
    "Suggestion: Use torch.utils.data.ConcatDataset to finalize the dataset. If you are more comfortable with numpy or another framework feel free to convert the data accordingly. <br><br>\n",
    "Example conversion from torch.tensor to numpy: `numpy_array = tensor.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First TODO: Combine datasets into one training and one test set\n",
    "# Suggested: use torch.utils.data.ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
