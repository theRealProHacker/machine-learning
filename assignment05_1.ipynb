{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.1 - Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Rashid Harvey and S M Shameem Ahmed Khan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 5.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1.1 - Binary Classification Evaluation\n",
    "\n",
    "* Use model implementations of `sklearn` (or other) for Logistic Regression and SVM for classification tasks. Train both models on the `Breast Cancer` dataset. (see given imports) **(RESULT)**\n",
    "* Evaluate the performance of both models using appropriate classification metrics and implement them using `numpy` only. Report at least on the following: accuracy, precision, recall, F1-score. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation\n",
      "------------------------------\n",
      "Accuracy : 0.9825\n",
      "Precision: 0.9861\n",
      "Recall   : 0.9861\n",
      "F1-score : 0.9861\n",
      "\n",
      "SVM Evaluation\n",
      "------------------------------\n",
      "Accuracy : 0.9737\n",
      "Precision: 0.9859\n",
      "Recall   : 0.9722\n",
      "F1-score : 0.9790\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "Y = data.target   # 0 = malignant, 1 = benign\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X, Y)\n",
    "\n",
    "# 80/20 train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "\n",
    "# Train the Models(Logistic Regression and SVM)\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "# SVM\n",
    "svm_clf = SVC(kernel=\"linear\")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predictions\n",
    "log_pred = log_reg.predict(X_test)\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Defining manual metric functions\n",
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Return TP, TN, FP, FN\"\"\"\n",
    "    # true positive\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    # true negative\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    # false positive\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    # false negative\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    # correct/total (like in a university exam)\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def precision(TP, FP):\n",
    "    # correct positive/total predicted positive\n",
    "    # i.e. how often is the model correct when it predicts positive\n",
    "    return TP / (TP + FP + 1e-12)  # avoiding division by zero\n",
    "\n",
    "def recall(TP, FN):\n",
    "    # true positives/actual positives\n",
    "    return TP / (TP + FN + 1e-12)\n",
    "\n",
    "def f1_score(prec, rec):\n",
    "    # https://en.wikipedia.org/wiki/F-score\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
    "    TP, TN, FP, FN = compute_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    acc = accuracy(TP, TN, FP, FN)\n",
    "    prec = precision(TP, FP)\n",
    "    rec = recall(TP, FN)\n",
    "    f1 = f1_score(prec, rec)\n",
    "\n",
    "    print(f\"\\n{name} Evaluation\")\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# Results\n",
    "evaluate_model(y_test, log_pred, \"Logistic Regression\")\n",
    "evaluate_model(y_test, svm_pred, \"SVM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1.2 - Multi-Class Classification Evaluation\n",
    "\n",
    "* Do the same as Task 5.1.1 for the multiclass problem `Iris`. Report on the performance metrics: accuracy, precision, recall, F1-score. **(RESULT)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9333\n",
      "Precision: 0.9333\n",
      "Recall   : 0.9333\n",
      "F1-score : 0.9333\n",
      "\n",
      "SVM Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 1.0000\n",
      "Precision: 1.0000\n",
      "Recall   : 1.0000\n",
      "F1-score : 1.0000\n"
     ]
    }
   ],
   "source": [
    "# multiclass\n",
    "# Setup code copied from above\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X, Y)\n",
    "\n",
    "# 80/20 train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# Train the Models(Logistic Regression and SVM)\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "# SVM\n",
    "svm_clf = SVC(kernel=\"linear\")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predictions\n",
    "log_pred = log_reg.predict(X_test)\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# new code\n",
    "def confusion_matrix_per_class(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Confusion matrix as above for each class but in a one-vs-rest manner.\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "    for cls in range(num_classes):\n",
    "        TP = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        TN = np.sum((y_true != cls) & (y_pred != cls))\n",
    "        FP = np.sum((y_true != cls) & (y_pred == cls))\n",
    "        FN = np.sum((y_true == cls) & (y_pred != cls))\n",
    "        matrices.append((TP, TN, FP, FN))\n",
    "    return matrices\n",
    "\n",
    "def aggregated_metrics(y_true, y_pred, num_classes):\n",
    "    matrices = confusion_matrix_per_class(y_true, y_pred, num_classes)\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "\n",
    "    for TP, TN, FP, FN in matrices:\n",
    "        prec = TP / (TP + FP + 1e-12)\n",
    "        rec = TP / (TP + FN + 1e-12)\n",
    "        f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(f1s)\n",
    "\n",
    "    return accuracy, precision, recall, f1, matrices\n",
    "\n",
    "# Evaluating Models\n",
    "num_classes = len(np.unique(Y))\n",
    "\n",
    "# Logistic Regression\n",
    "log_acc, log_prec, log_rec, log_f1, log_matrices = aggregated_metrics(y_test, log_pred, num_classes)\n",
    "print(f\"\\nLogistic Regression Evaluation (Average)\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Accuracy : {log_acc:.4f}\")\n",
    "print(f\"Precision: {log_prec:.4f}\")\n",
    "print(f\"Recall   : {log_rec:.4f}\")\n",
    "print(f\"F1-score : {log_f1:.4f}\")\n",
    "\n",
    "# SVM\n",
    "svm_acc, svm_prec, svm_rec, svm_f1, svm_matrices = aggregated_metrics(y_test, svm_pred, num_classes)\n",
    "print(f\"\\nSVM Evaluation (Average)\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Accuracy : {svm_acc:.4f}\")\n",
    "print(f\"Precision: {svm_prec:.4f}\")\n",
    "print(f\"Recall   : {svm_rec:.4f}\")\n",
    "print(f\"F1-score : {svm_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1.3 - Regression Evaluation\n",
    "\n",
    "* Now evaluate a trained `Linear Regression` and `SVM` model for the Regression task `Diabetes`. Report on the performance metrics: MSE, RMSE, MAE, RÂ². **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Evaluation\n",
      "-----------------------------\n",
      "MSE : 2900.1936\n",
      "RMSE: 53.8534\n",
      "MAE : 42.7941\n",
      "R2  : 0.4526\n",
      "\n",
      "SVM Regression Evaluation\n",
      "-----------------------------\n",
      "MSE : 2938.4619\n",
      "RMSE: 54.2076\n",
      "MAE : 43.3242\n",
      "R2  : 0.4454\n",
      "\n",
      "Reuslt: Linear regression performs better than SVM regression,\n",
      "even though SVM was better for classification.\n"
     ]
    }
   ],
   "source": [
    "# regression\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X, Y)\n",
    "\n",
    "# 80/20 train-test split (not stratified, because regression)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the Models\n",
    "# Linear (instead of logistic) regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "# SVM (but for regression)\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predictions\n",
    "lin_pred = lin_reg.predict(X_test)\n",
    "svm_pred = svm_reg.predict(X_test)\n",
    "\n",
    "# metrics\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "# mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    return 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
    "    mse_val = mse(y_true, y_pred)\n",
    "    rmse_val = rmse(y_true, y_pred)\n",
    "    mae_val = mae(y_true, y_pred)\n",
    "    r2_val = r2(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n{name} Evaluation\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(f\"MSE : {mse_val:.4f}\")\n",
    "    print(f\"RMSE: {rmse_val:.4f}\")\n",
    "    print(f\"MAE : {mae_val:.4f}\")\n",
    "    print(f\"R2  : {r2_val:.4f}\")\n",
    "\n",
    "evaluate_model(y_test, lin_pred, \"Linear Regression\")\n",
    "evaluate_model(y_test, svm_pred, \"SVM Regression\")\n",
    "\n",
    "print()\n",
    "print(\"Reuslt: Linear regression performs better than SVM regression,\\neven though SVM was better for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1.4 - Cross-Validation (BONUS)\n",
    "\n",
    "* Set up a cross-validation pipeline for the `Linear Regression` and `SVM` models on the `Diabetes` dataset. (Regression) **(RESULT)**\n",
    "* Set up a cross-validation pipeline for the `Logistic Regression` and `SVM` models on the `Iris` dataset. (Classification) **(RESULT)**\n",
    "* Report the performance metrics on all folds (minimum 5-fold) for each model and dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Fold 1 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0641\n",
      "RMSE: 0.2532\n",
      "MAE : 0.1969\n",
      "R2  : 0.7271\n",
      "\n",
      "SVM Regression Fold 1 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0687\n",
      "RMSE: 0.2620\n",
      "MAE : 0.1980\n",
      "R2  : 0.7077\n",
      "\n",
      "Linear Regression Fold 2 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0569\n",
      "RMSE: 0.2384\n",
      "MAE : 0.1854\n",
      "R2  : 0.7474\n",
      "\n",
      "SVM Regression Fold 2 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0574\n",
      "RMSE: 0.2395\n",
      "MAE : 0.1811\n",
      "R2  : 0.7452\n",
      "\n",
      "Linear Regression Fold 3 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0540\n",
      "RMSE: 0.2324\n",
      "MAE : 0.1800\n",
      "R2  : 0.7721\n",
      "\n",
      "SVM Regression Fold 3 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0599\n",
      "RMSE: 0.2448\n",
      "MAE : 0.1872\n",
      "R2  : 0.7472\n",
      "\n",
      "Linear Regression Fold 4 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0912\n",
      "RMSE: 0.3019\n",
      "MAE : 0.2278\n",
      "R2  : 0.6119\n",
      "\n",
      "SVM Regression Fold 4 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0853\n",
      "RMSE: 0.2921\n",
      "MAE : 0.2196\n",
      "R2  : 0.6369\n",
      "\n",
      "Linear Regression Fold 5 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0487\n",
      "RMSE: 0.2208\n",
      "MAE : 0.1738\n",
      "R2  : 0.7807\n",
      "\n",
      "SVM Regression Fold 5 Evaluation\n",
      "-----------------------------\n",
      "MSE : 0.0542\n",
      "RMSE: 0.2328\n",
      "MAE : 0.1748\n",
      "R2  : 0.7560\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation means doing many different train-test splits\n",
    "\n",
    "# Most code copied\n",
    "\n",
    "def cross_validate_diabetes(X, Y, num_folds=5):\n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "\n",
    "    _random_state = 42  # fixed seed for reproducibility\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        random_state = fold * 10 + _random_state\n",
    "\n",
    "        # 80/20 train-test split (not stratified, because regression)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, Y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Train the Models\n",
    "        # Linear (instead of logistic) regression\n",
    "        lin_reg = LinearRegression()\n",
    "        lin_reg.fit(X_train, y_train)\n",
    "        # SVM (but for regression)\n",
    "        svm_reg = SVR(kernel=\"linear\")\n",
    "        svm_reg.fit(X_train, y_train)\n",
    "\n",
    "        # predictions\n",
    "        lin_pred = lin_reg.predict(X_test)\n",
    "        svm_pred = svm_reg.predict(X_test)\n",
    "\n",
    "        # using metrics defined above\n",
    "\n",
    "        # Evaluation\n",
    "        def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
    "            mse_val = mse(y_true, y_pred)\n",
    "            rmse_val = rmse(y_true, y_pred)\n",
    "            mae_val = mae(y_true, y_pred)\n",
    "            r2_val = r2(y_true, y_pred)\n",
    "\n",
    "            print(f\"\\n{name} Evaluation\")\n",
    "            print(\"-----------------------------\")\n",
    "            print(f\"MSE : {mse_val:.4f}\")\n",
    "            print(f\"RMSE: {rmse_val:.4f}\")\n",
    "            print(f\"MAE : {mae_val:.4f}\")\n",
    "            print(f\"R2  : {r2_val:.4f}\")\n",
    "\n",
    "        evaluate_model(y_test, lin_pred, f\"Linear Regression Fold {fold+1}\")\n",
    "        evaluate_model(y_test, svm_pred, f\"SVM Regression Fold {fold+1}\")\n",
    "\n",
    "cross_validate_diabetes(X, Y, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Fold 1 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9737\n",
      "Precision: 0.9742\n",
      "Recall   : 0.9697\n",
      "F1-score : 0.9719\n",
      "\n",
      "SVM Fold 1 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9561\n",
      "Precision: 0.9516\n",
      "Recall   : 0.9556\n",
      "F1-score : 0.9535\n",
      "\n",
      "Logistic Regression Fold 2 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9912\n",
      "Precision: 0.9934\n",
      "Recall   : 0.9872\n",
      "F1-score : 0.9902\n",
      "\n",
      "SVM Fold 2 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9737\n",
      "Precision: 0.9682\n",
      "Recall   : 0.9738\n",
      "F1-score : 0.9709\n",
      "\n",
      "Logistic Regression Fold 3 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9825\n",
      "Precision: 0.9861\n",
      "Recall   : 0.9773\n",
      "F1-score : 0.9813\n",
      "\n",
      "SVM Fold 3 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9825\n",
      "Precision: 0.9861\n",
      "Recall   : 0.9773\n",
      "F1-score : 0.9813\n",
      "\n",
      "Logistic Regression Fold 4 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9649\n",
      "Precision: 0.9673\n",
      "Recall   : 0.9581\n",
      "F1-score : 0.9623\n",
      "\n",
      "SVM Fold 4 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9474\n",
      "Precision: 0.9482\n",
      "Recall   : 0.9394\n",
      "F1-score : 0.9435\n",
      "\n",
      "Logistic Regression Fold 5 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9912\n",
      "Precision: 0.9935\n",
      "Recall   : 0.9868\n",
      "F1-score : 0.9901\n",
      "\n",
      "SVM Fold 5 Evaluation (Average)\n",
      "------------------------------\n",
      "Accuracy : 0.9737\n",
      "Precision: 0.9810\n",
      "Recall   : 0.9605\n",
      "F1-score : 0.9698\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_validate_iris(X, Y, num_folds=5):\n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "\n",
    "    _random_state = 42  # fixed seed for reproducibility\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        random_state = fold * 10 + _random_state\n",
    "\n",
    "        # 80/20 train-test split (not stratified, because regression)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, Y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Train the Models(Logistic Regression and SVM)\n",
    "        # Logistic Regression\n",
    "        log_reg = LogisticRegression(max_iter=500)\n",
    "        log_reg.fit(X_train, y_train)\n",
    "        # SVM\n",
    "        svm_clf = SVC(kernel=\"linear\")\n",
    "        svm_clf.fit(X_train, y_train)\n",
    "\n",
    "        # predictions\n",
    "        log_pred = log_reg.predict(X_test)\n",
    "        svm_pred = svm_clf.predict(X_test)\n",
    "\n",
    "        def confusion_matrix_per_class(y_true, y_pred, num_classes):\n",
    "            \"\"\"\n",
    "            Confusion matrix as above for each class but in a one-vs-rest manner.\n",
    "            \"\"\"\n",
    "            matrices = []\n",
    "            for cls in range(num_classes):\n",
    "                TP = np.sum((y_true == cls) & (y_pred == cls))\n",
    "                TN = np.sum((y_true != cls) & (y_pred != cls))\n",
    "                FP = np.sum((y_true != cls) & (y_pred == cls))\n",
    "                FN = np.sum((y_true == cls) & (y_pred != cls))\n",
    "                matrices.append((TP, TN, FP, FN))\n",
    "            return matrices\n",
    "\n",
    "        def evaluate_model(y_true, y_pred, num_classes, name=\"Model\"):\n",
    "            matrices = confusion_matrix_per_class(y_true, y_pred, num_classes)\n",
    "            precisions, recalls, f1s = [], [], []\n",
    "\n",
    "            for TP, TN, FP, FN in matrices:\n",
    "                prec = TP / (TP + FP + 1e-12)\n",
    "                rec = TP / (TP + FN + 1e-12)\n",
    "                f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "                precisions.append(prec)\n",
    "                recalls.append(rec)\n",
    "                f1s.append(f1)\n",
    "\n",
    "            accuracy = np.mean(y_true == y_pred)\n",
    "            precision = np.mean(precisions)\n",
    "            recall = np.mean(recalls)\n",
    "            f1 = np.mean(f1s)\n",
    "\n",
    "            print(f\"\\n{name} Fold {fold+1} Evaluation (Average)\")\n",
    "            print(\"-\"*30)\n",
    "            print(f\"Accuracy : {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall   : {recall:.4f}\")\n",
    "            print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "        num_classes = len(np.unique(Y))\n",
    "        evaluate_model(y_test, log_pred, num_classes, name=\"Logistic Regression\")\n",
    "        evaluate_model(y_test, svm_pred, num_classes, name=\"SVM\")\n",
    "            \n",
    "\n",
    "cross_validate_iris(X, Y, num_folds=5)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
